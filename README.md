# NRGETC
Code for our paper 
# "A News Recommendation Framework Utilizing ChatGPT: Estimating Target Audience and News Categories"
![Proposed News Recommendation Framework Illustration](pf.png)

# Get started
Basic setup.
```bash
git clone https://github.com/Maekawa-Y/NRGETC.git
cd NRGETC
pip install -r requirements.txt
```

Download and preprocess the data.
```bash
mkdir data && cd data
# Download GloVe pre-trained word embedding
wget https://nlp.stanford.edu/data/glove.840B.300d.zip
sudo apt install unzip
unzip glove.840B.300d.zip -d glove
rm glove.840B.300d.zip

# Download MIND dataset
# By downloading the dataset, you agree to the [Microsoft Research License Terms](https://go.microsoft.com/fwlink/?LinkID=206977). For more detail about the dataset, see https://msnews.github.io/.
# Uncomment the following lines to use the MIND Small dataset (Note MIND Small doesn't have a test set, In our paper we use the validation data from the original dataset as test data in this experiment.
# In addition, the training data in the original dataset are divided into the training data (first 5 days) and the validation data (last 1 day) for this experiment. So you need to do the splitting of the data set after downloading:)
wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip
unzip MINDsmall_train.zip -d train
unzip MINDsmall_dev.zip -d val
rm MINDsmall_*.zip

# Description of folder structure.
```
📦 this-repo-name
├── 📁 src                                                  # Main source code of the experiment
|   ├── 📁 model           
|   |   ├── 📁 general
|   |   |   ├── 📁 attention
|   |   |   |   ├── 📄 additive.py
|   |   |   |   ├── 📄 info_attention.py
|   |   |   |   ├── 📄 masked_softmax.py
|   |   |   |   ├── 📄 multihead_self.py
|   |   |   |   └── 📄 personalized_attention.py
|   |   |   └── 📁 click_predictor
|   |   |       ├── 📄 DNN.py
|   |   |       └── 📄 dot_product.py
|   |   ├── 📁 NRMS
|   |   |   ├── 📄 __init__.py
|   |   |   ├── 📄 news_encoder.py
|   |   |   └── 📄 user_encoder.py
|   |   ├── 📁 NAML
|   |   |   ├── 📄 __init__.py
|   |   |   ├── 📄 news_encoder.py
|   |   |   └── 📄 user_encoder.py
|   |   ├── 📁 LSTUR
|   |   |   ├── 📄 __init__.py
|   |   |   ├── 📄 news_encoder.py
|   |   |   └── 📄 user_encoder.py
|   |   └── 📁 NPA
|   |       ├── 📄 __init__.py
|   |       ├── 📄 news_encoder.py
|   |       └── 📄 user_encoder.py
|   ├── 📄 config.py                                       # File for setting model parameters, etc.
|   ├── 📄 data_preprocess.py                              # Data preprocessing file
|   ├── 📄 dataset.py                                      # Files that convert data into a form that can be input into a model.
|   ├── 📄 evaluate.py                                     # File to experiment with valuation data at the end of each epoch in the learning process. (NRMS, NAML, LSTUR)
|   ├── 📄 test.py                                         # Files for testing (NRMS, NAML, LSTUR)
|   ├── 📄 NPA_evaluate.py                                 # File to experiment with valuation data at the end of each epoch in the learning process. (NPA)
|   ├── 📄 NPA_test.py                                     # Files for testing (NPA)
|   ├── 📄 view.py                                         # Files for drawing
|   ├── 📄 train_NRMS.ipynb                                # File for model training and validation.(NRMS)
|   ├── 📄 train_NAML.ipynb                                # File for model training and validation.(NAML)
|   ├── 📄 train_LSTUR.ipynb                               # File for model training and validation.(LSTUR)
|   ├── 📄 train_NPA.ipynb                                 # File for model training and validation.(NPA)
│   └── 📁 (path)                                          # Save the best parameters in the training of each model.
├── 📁 (data)                                              # This folder must be created according to the Data Download instructions below.
├── 📁 Generated_data                                      # Stores information generated by GPT (targets and categories)
    ├── 📄 Generated_Target.csv   
│   └── 📄 Generated_Category.csv
├── 📁 GPT_Augmentation                                    # Information extension using ChatGPT's API.
    ├── 📄 GPT_API.ipynb                                   # Files that extend information by accessing ChatGPT using the API.
│   └── 📄 all_news_title.csv
├── 📄 README.md          
└── 📄 requirements.txt   
(〇〇) represents the folder that needs to be created when using this code. Not in this current repository.
*** By changing the information of the inputs to the model (e.g. adding targets or categories acquired from the GPT) in the src folder model, various experimental models can be conducted. ***
```
