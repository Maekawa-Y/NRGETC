# NRGETC
Code for our paper 
# "A News Recommendation Framework Utilizing ChatGPT: Estimating Target Audience and News Categories"
![Proposed News Recommendation Framework Illustration](pf.png)

# Get started
Basic setup.
```bash
git clone https://github.com/Maekawa-Y/NRGETC.git
cd NRGETC
pip install -r requirements.txt
```

Download and preprocess the data.
```bash
mkdir data && cd data
# Download GloVe pre-trained word embedding
wget https://nlp.stanford.edu/data/glove.840B.300d.zip
sudo apt install unzip
unzip glove.840B.300d.zip -d glove
rm glove.840B.300d.zip

# Download MIND dataset
# By downloading the dataset, you agree to the [Microsoft Research License Terms](https://go.microsoft.com/fwlink/?LinkID=206977). For more detail about the dataset, see https://msnews.github.io/.
# Uncomment the following lines to use the MIND Small dataset (Note MIND Small doesn't have a test set, In our paper we use the validation data from the original dataset as test data in this experiment.
# In addition, the training data in the original dataset are divided into the training data (first 5 days) and the validation data (last 1 day) for this experiment. So you need to do the splitting of the data set after downloading:)
wget https://mind201910small.blob.core.windows.net/release/MINDsmall_train.zip https://mind201910small.blob.core.windows.net/release/MINDsmall_dev.zip
unzip MINDsmall_train.zip -d train
unzip MINDsmall_dev.zip -d val
rm MINDsmall_*.zip

# Description of folder structure.
```
ğŸ“¦ this-repo-name
â”œâ”€â”€ ğŸ“ src                                                  # Main source code of the experiment
|   â”œâ”€â”€ ğŸ“ model           
|   |   â”œâ”€â”€ ğŸ“ general
|   |   |   â”œâ”€â”€ ğŸ“ attention
|   |   |   |   â”œâ”€â”€ ğŸ“„ additive.py
|   |   |   |   â”œâ”€â”€ ğŸ“„ info_attention.py
|   |   |   |   â”œâ”€â”€ ğŸ“„ masked_softmax.py
|   |   |   |   â”œâ”€â”€ ğŸ“„ multihead_self.py
|   |   |   |   â””â”€â”€ ğŸ“„ personalized_attention.py
|   |   |   â””â”€â”€ ğŸ“ click_predictor
|   |   |       â”œâ”€â”€ ğŸ“„ DNN.py
|   |   |       â””â”€â”€ ğŸ“„ dot_product.py
|   |   â”œâ”€â”€ ğŸ“ NRMS
|   |   |   â”œâ”€â”€ ğŸ“„ __init__.py
|   |   |   â”œâ”€â”€ ğŸ“„ news_encoder.py
|   |   |   â””â”€â”€ ğŸ“„ user_encoder.py
|   |   â”œâ”€â”€ ğŸ“ NAML
|   |   |   â”œâ”€â”€ ğŸ“„ __init__.py
|   |   |   â”œâ”€â”€ ğŸ“„ news_encoder.py
|   |   |   â””â”€â”€ ğŸ“„ user_encoder.py
|   |   â”œâ”€â”€ ğŸ“ LSTUR
|   |   |   â”œâ”€â”€ ğŸ“„ __init__.py
|   |   |   â”œâ”€â”€ ğŸ“„ news_encoder.py
|   |   |   â””â”€â”€ ğŸ“„ user_encoder.py
|   |   â””â”€â”€ ğŸ“ NPA
|   |       â”œâ”€â”€ ğŸ“„ __init__.py
|   |       â”œâ”€â”€ ğŸ“„ news_encoder.py
|   |       â””â”€â”€ ğŸ“„ user_encoder.py
|   â”œâ”€â”€ ğŸ“„ config.py                                       # File for setting model parameters, etc.
|   â”œâ”€â”€ ğŸ“„ data_preprocess.py                              # Data preprocessing file
|   â”œâ”€â”€ ğŸ“„ dataset.py                                      # Files that convert data into a form that can be input into a model.
|   â”œâ”€â”€ ğŸ“„ evaluate.py                                     # File to experiment with valuation data at the end of each epoch in the learning process. (NRMS, NAML, LSTUR)
|   â”œâ”€â”€ ğŸ“„ test.py                                         # Files for testing (NRMS, NAML, LSTUR)
|   â”œâ”€â”€ ğŸ“„ NPA_evaluate.py                                 # File to experiment with valuation data at the end of each epoch in the learning process. (NPA)
|   â”œâ”€â”€ ğŸ“„ NPA_test.py                                     # Files for testing (NPA)
|   â”œâ”€â”€ ğŸ“„ view.py                                         # Files for drawing
|   â”œâ”€â”€ ğŸ“„ train_NRMS.ipynb                                # File for model training and validation.(NRMS)
|   â”œâ”€â”€ ğŸ“„ train_NAML.ipynb                                # File for model training and validation.(NAML)
|   â”œâ”€â”€ ğŸ“„ train_LSTUR.ipynb                               # File for model training and validation.(LSTUR)
|   â”œâ”€â”€ ğŸ“„ train_NPA.ipynb                                 # File for model training and validation.(NPA)
â”‚   â””â”€â”€ ğŸ“ (path)                                          # Save the best parameters in the training of each model.
â”œâ”€â”€ ğŸ“ (data)                                              # This folder must be created according to the Data Download instructions below.
â”œâ”€â”€ ğŸ“ Generated_data                                      # Stores information generated by GPT (targets and categories)
    â”œâ”€â”€ ğŸ“„ Generated_Target.csv   
â”‚   â””â”€â”€ ğŸ“„ Generated_Category.csv
â”œâ”€â”€ ğŸ“ GPT_Augmentation                                    # Information extension using ChatGPT's API.
    â”œâ”€â”€ ğŸ“„ GPT_API.ipynb                                   # Files that extend information by accessing ChatGPT using the API.
â”‚   â””â”€â”€ ğŸ“„ all_news_title.csv
â”œâ”€â”€ ğŸ“„ README.md          
â””â”€â”€ ğŸ“„ requirements.txt   
(ã€‡ã€‡) represents the folder that needs to be created when using this code. Not in this current repository.
*** By changing the information of the inputs to the model (e.g. adding targets or categories acquired from the GPT) in the src folder model, various experimental models can be conducted. ***
```
